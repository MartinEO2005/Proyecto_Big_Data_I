#!/usr/bin/env python3
"""
neo_lumina_by_theme.py

Busca metadatos Copernicus (Sentinel-2 L2A y Sentinel-1 GRD) vía OData,
extrae estaciones ferroviarias vía Overpass (OSM) y genera CSVs temáticos
(listos para luego descargar datos y procesar para ML).

Uso:
  - Define variables de entorno (o edita las defaults abajo).
  - Ejecuta: python neo_lumina_by_theme.py

Dependencias:
  pip install requests pandas shapely
"""

import os
import json
import requests
import pandas as pd
from datetime import datetime
from shapely import wkt

# ---------------------------
# Parámetros (puedes sobreescribir con VARIABLES DE ENTORNO)
# ---------------------------
COLLECTION_S2 = os.getenv("COLLECTION_S2", "SENTINEL-2")   # para L2A
COLLECTION_S1 = os.getenv("COLLECTION_S1", "SENTINEL-1")   # para GRD
DATE_FROM = os.getenv("DATE_FROM", "2024-01-01")
DATE_TO   = os.getenv("DATE_TO", "2024-12-31")
AOI_WKT   = os.getenv("AOI_WKT", None)  # opcional
MAX_CLOUD = os.getenv("MAX_CLOUD", None)  # sólo aplica S2
OUTDIR    = os.getenv("OUTDIR", "./neo_lumina_output")
TOP       = int(os.getenv("TOP", "100"))  # seguro para OData

os.makedirs(OUTDIR, exist_ok=True)

CATALOG_BASE = "https://catalogue.dataspace.copernicus.eu/odata/v1/Products"

# ---------------------------
# Helpers OData (Copernicus)
# ---------------------------
def build_filter(collection, date_from, date_to, aoi_wkt=None, cloud=None):
    # Mapeo colecciones correctas
    collection_map = {
        "SENTINEL-2": "S2MSI2A",
        "SENTINEL-1": "S1_GRD"
    }
    collection = collection_map.get(collection.upper(), collection)

    filt = f"Collection/Name eq '{collection}' and ContentDate/Start ge {date_from}T00:00:00Z and ContentDate/Start le {date_to}T23:59:59Z"

    if aoi_wkt:
        filt = f"Collection/Name eq '{collection}' and OData.CSC.Intersects(area=geography'SRID=4326;{aoi_wkt}') and ContentDate/Start ge {date_from}T00:00:00Z and ContentDate/Start le {date_to}T23:59:59Z"

    if cloud is not None and "SENTINEL-2" in collection.upper():
        try:
            cloud_val = int(cloud)
            filt += f" and Attributes/any(a: a/Name eq 'CloudCover' and a/Value le {cloud_val})"
        except ValueError:
            print(f"[WARN] MAX_CLOUD no es un número válido: {cloud}, ignorando filtro CloudCover.")
    return filt

def query_catalog(filter_expr, top=100, max_pages=50):
    items = []
    q = f"{CATALOG_BASE}?$filter={filter_expr}&$count=true&$top={top}"
    headers = {"Accept": "application/json"}
    page = 0

    while q:
        page += 1
        print(f"[OData] Página {page}: {q}")
        r = requests.get(q, headers=headers, timeout=60)
        if r.status_code != 200:
            print(f"[ERROR] Status {r.status_code}: {r.text[:500]}")
            r.raise_for_status()

        js = r.json()
        page_items = js.get("value", [])
        if not page_items:
            print("[OData] No hay resultados en esta página.")
            break
        items.extend(page_items)

        q = js.get("@odata.nextLink")
        if page >= max_pages:
            print(f"[OData] Límite de páginas alcanzado ({max_pages}).")
            break

    print(f"[OData] Total de productos obtenidos: {len(items)}")
    return items

def items_to_selected_df(items, theme):
    if not items:
        return pd.DataFrame()
    df = pd.json_normalize(items)
    if "ContentDate.Start" in df.columns:
        df["content_start"] = pd.to_datetime(df["ContentDate.Start"])
    else:
        df["content_start"] = pd.to_datetime(df.get("ContentDate", pd.NA).apply(lambda x: x.get("Start") if isinstance(x, dict) else None))

    df["footprint"] = df.get("GeoFootprint", None)

    def find_attr(attrs, keyname):
        if isinstance(attrs, list):
            for a in attrs:
                if isinstance(a, dict):
                    name = str(a.get("Name","")).lower()
                    if keyname.lower() in name:
                        return a.get("Value") or a.get("value") or a.get("doubleValue") or a.get("stringValue") or None
        return None

    df["identifier"] = df["Name"].astype(str).str.split(".").str[0]
    df["cloudcover"] = df.get("CloudCover", None)
    if df["cloudcover"].isnull().all():
        df["cloudcover"] = df.get("Attributes", None).apply(lambda a: find_attr(a, "cloud"))

    if theme == "sentinel2":
        df["collection"] = COLLECTION_S2
        df["product_level"] = df.get("Name", "").astype(str).str.contains("L2A").map({True:"L2A", False:"unknown"})
        df["s3_asset"] = df.get("Assets", None).apply(lambda a: json.dumps(a) if a else None)
        keep = ["Id","Name","identifier","content_start","IngestionDate","cloudcover","sizeInBytes","footprint","product_level","s3_asset"]
    else:
        df["collection"] = COLLECTION_S1
        df["polarizations"] = df.get("Attributes", None).apply(lambda a: find_attr(a, "polar") if a else None)
        df["s3_asset"] = df.get("Assets", None).apply(lambda a: json.dumps(a) if a else None)
        keep = ["Id","Name","identifier","content_start","IngestionDate","polarizations","sizeInBytes","footprint","s3_asset"]

    cols = [c for c in keep if c in df.columns]
    return df[cols]

# ---------------------------
# Overpass (OSM)
# ---------------------------
OVERPASS_URL = "https://overpass-api.de/api/interpreter"

def bbox_from_wkt(aoi_wkt):
    geom = wkt.loads(aoi_wkt)
    minx, miny, maxx, maxy = geom.bounds
    return miny, minx, maxy, maxx

def fetch_rail_stations(aoi_wkt=None):
    if not aoi_wkt:
        raise RuntimeError("AOI_WKT es obligatorio para buscar estaciones rail.")
    s, w, n, e = bbox_from_wkt(aoi_wkt)
    query = f"""
    [out:json][timeout:60];
    (
      node["railway"="station"]({s},{w},{n},{e});
      node["station"="rail"]({s},{w},{n},{e});
      way["railway"="station"]({s},{w},{n},{e});
      relation["railway"="station"]({s},{w},{n},{e});
    );
    out center tags;
    """
    print("[Overpass] Querying stations within bbox:", (s,w,n,e))
    r = requests.post(OVERPASS_URL, data={"data": query}, timeout=120)
    r.raise_for_status()
    js = r.json()
    elements = js.get("elements", [])
    rows = []
    for el in elements:
        obj_id = el.get("id")
        tags = el.get("tags", {})
        lat = el.get("lat") or (el.get("center") or {}).get("lat")
        lon = el.get("lon") or (el.get("center") or {}).get("lon")
        rows.append({
            "osm_id": obj_id,
            "name": tags.get("name"),
            "lat": lat,
            "lon": lon,
            "tags": json.dumps(tags, ensure_ascii=False)
        })
    return pd.DataFrame(rows)

# ---------------------------
# VIIRS template
# ---------------------------
def create_viirs_template(outdir, date_from, date_to, aoi_wkt=None):
    rows = [{
        "variable": "VIIRS_DNB_monthly_radiance",
        "date_from": date_from,
        "date_to": date_to,
        "aoi_wkt": aoi_wkt,
        "download_url": "",
        "notes": "Rellenar con enlaces VIIRS VNL (NOAA / Earth Observation Group)."
    }]
    df = pd.DataFrame(rows)
    path = os.path.join(outdir, "viirs_requests.csv")
    df.to_csv(path, index=False)
    return path

# ---------------------------
# Main
# ---------------------------
def main():
    print("=== NeoLumina: Generador de CSV temáticos ===")
    print(f"AOI_WKT set? {bool(AOI_WKT)}, DATE_FROM={DATE_FROM}, DATE_TO={DATE_TO}, TOP={TOP}, OUTDIR={OUTDIR}\n")

    # 1) Sentinel-2
    print("\n==> 1) Sentinel-2 (L2A) metadatos via OData ...")
    filt2 = build_filter(COLLECTION_S2, DATE_FROM, DATE_TO, aoi_wkt=AOI_WKT, cloud=MAX_CLOUD)
    items_s2 = query_catalog(filt2, top=TOP)
    df_s2 = items_to_selected_df(items_s2, theme="sentinel2")
    out_s2 = os.path.join(OUTDIR, "sentinel2_products.csv")
    df_s2.to_csv(out_s2, index=False)
    print(f"[OK] Sentinel-2 CSV guardado: {out_s2} (rows: {len(df_s2)})")

    # 2) Sentinel-1
    print("\n==> 2) Sentinel-1 (GRD) metadatos via OData ...")
    filt1 = build_filter(COLLECTION_S1, DATE_FROM, DATE_TO, aoi_wkt=AOI_WKT)
    items_s1 = query_catalog(filt1, top=TOP)
    df_s1 = items_to_selected_df(items_s1, theme="sentinel1")
    out_s1 = os.path.join(OUTDIR, "sentinel1_products.csv")
    df_s1.to_csv(out_s1, index=False)
    print(f"[OK] Sentinel-1 CSV guardado: {out_s1} (rows: {len(df_s1)})")

    # 3) Rail stations
    if AOI_WKT:
        try:
            print("\n==> 3) Extrayendo estaciones ferroviarias (OSM) ...")
            df_rail = fetch_rail_stations(AOI_WKT)
            out_rail = os.path.join(OUTDIR, "rail_stations.csv")
            df_rail.to_csv(out_rail, index=False)
            print(f"[OK] Rail CSV guardado: {out_rail} (rows: {len(df_rail)})")
        except Exception as e:
            print("⚠️ Error obteniendo estaciones rail:", e)
    else:
        print("\n==> 3) Rail stations: AOI_WKT no proporcionada. Saltando.")

    # 4) VIIRS template
    viirs_path = create_viirs_template(OUTDIR, DATE_FROM, DATE_TO, AOI_WKT)
    print(f"\n[INFO] Plantilla VIIRS guardada en: {viirs_path}")

    print("\n=== Proceso completado. CSVs generados en:", OUTDIR, "===")

if __name__ == "__main__":
    main()
